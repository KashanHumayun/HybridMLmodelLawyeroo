{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Fake Data that will be used in our recommendation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "%pip install faker\n",
    "%pip install tensorflow\n",
    "%pip install tensorflow_recommenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = Faker()\n",
    "\n",
    "# Function to generate fake lawyer data\n",
    "lawyer_id_counter = 1  # Initialize a counter\n",
    "\n",
    "def generate_lawyer_data(num_lawyers):\n",
    "    lawyer_data = []\n",
    "    for _ in range(num_lawyers):\n",
    "        lawyer_data.append({\n",
    "            'lawyer_id': fake.uuid4(),\n",
    "            'first_name': fake.first_name(),\n",
    "            'last_name': fake.last_name(),\n",
    "            'email': fake.email(),\n",
    "            'ph_number': fake.phone_number(),\n",
    "            'address': fake.address(),\n",
    "            'password': fake.password(),\n",
    "            'specializations': random.sample(['Personal Injury', 'Criminal', 'Family', 'Immigration', 'Business', 'Estate Planning'], k=random.randint(1, 3)),\n",
    "            'years_of_experience': random.randint(1, 30),\n",
    "            'universities': fake.company(),\n",
    "            'rating': round(random.uniform(1, 5), 2),\n",
    "            'created_at': fake.date_time_this_decade(),\n",
    "            'updated_at': fake.date_time_this_decade(),\n",
    "            'profile_picture': fake.image_url(),\n",
    "            'verified': fake.boolean(),\n",
    "            'account_type': 'lawyer'\n",
    "        })\n",
    "    return pd.DataFrame(lawyer_data)\n",
    "\n",
    "# Function to generate fake client data\n",
    "def generate_client_data(num_clients):\n",
    "    client_data = []\n",
    "    for _ in range(num_clients):\n",
    "        client_data.append({\n",
    "            'client_id': fake.uuid4(),\n",
    "            'first_name': fake.first_name(),\n",
    "            'last_name': fake.last_name(),\n",
    "            'email': fake.email(),\n",
    "            'ph_number': fake.phone_number(),\n",
    "            'address': fake.address(),\n",
    "            'password': fake.password(),\n",
    "            'created_at': fake.date_time_this_decade(),\n",
    "            'updated_at': fake.date_time_this_decade(),\n",
    "            'profile_picture': fake.image_url(),\n",
    "            'verified': fake.boolean(),\n",
    "            'account_type': 'client',\n",
    "            'preferences': random.sample(['Personal Injury', 'Criminal', 'Family', 'Immigration', 'Business', 'Estate Planning'], k=random.randint(1, 3))\n",
    "        })\n",
    "    return pd.DataFrame(client_data)\n",
    "\n",
    "# Function to generate fake client interactions\n",
    "def generate_client_interactions(num_interactions, lawyer_ids, client_ids):\n",
    "    interaction_data = []\n",
    "    for _ in range(num_interactions):\n",
    "        interaction_data.append({\n",
    "            'interaction_id': fake.uuid4(),\n",
    "            'interaction_type': random.choice(['meeting', 'call', 'message']),\n",
    "            'timestamp': fake.date_time_this_year(),\n",
    "            'lawyer_id': random.choice(lawyer_ids),\n",
    "            'client_id': random.choice(client_ids),\n",
    "            'context': fake.sentence()\n",
    "        })\n",
    "    return pd.DataFrame(interaction_data)\n",
    "\n",
    "# Function to generate fake lawyer ratings\n",
    "def generate_lawyer_ratings(num_ratings, lawyer_ids, client_ids):\n",
    "    rating_data = []\n",
    "    for _ in range(num_ratings):\n",
    "        rating_data.append({\n",
    "            'rating_id': fake.uuid4(),\n",
    "            'client_id': random.choice(client_ids),\n",
    "            'lawyer_id': random.choice(lawyer_ids),\n",
    "            'ratings': round(random.uniform(1, 5), 2),\n",
    "            'created_at': fake.date_time_this_decade()\n",
    "        })\n",
    "    return pd.DataFrame(rating_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data for lawyers and clients\n",
    "num_lawyers = 100\n",
    "num_clients = 200\n",
    "\n",
    "lawyer_data = generate_lawyer_data(num_lawyers)\n",
    "client_data = generate_client_data(num_clients)\n",
    "# Generate data for interactions and ratings\n",
    "interaction_data = generate_client_interactions(num_interactions=500, lawyer_ids=lawyer_data['lawyer_id'].unique(), client_ids=client_data['client_id'].unique())\n",
    "rating_data = generate_lawyer_ratings(num_ratings=300, lawyer_ids=lawyer_data['lawyer_id'].unique(), client_ids=client_data['client_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the interaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lawyer Data:\")\n",
    "lawyer_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nClient Data:\")\n",
    "client_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 2s 76ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.2000 - factorized_top_k/top_10_categorical_accuracy: 0.3000 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 22.6878 - regularization_loss: 0.0000e+00 - total_loss: 22.6878\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 64ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1500 - factorized_top_k/top_5_categorical_accuracy: 0.6000 - factorized_top_k/top_10_categorical_accuracy: 0.9000 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 14.8854 - regularization_loss: 0.0000e+00 - total_loss: 14.8854\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 68ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2500 - factorized_top_k/top_5_categorical_accuracy: 0.7000 - factorized_top_k/top_10_categorical_accuracy: 0.9500 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 9.2346 - regularization_loss: 0.0000e+00 - total_loss: 9.2346\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 66ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1500 - factorized_top_k/top_5_categorical_accuracy: 0.7000 - factorized_top_k/top_10_categorical_accuracy: 1.0000 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 4.2023 - regularization_loss: 0.0000e+00 - total_loss: 4.2023\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 67ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1500 - factorized_top_k/top_5_categorical_accuracy: 0.7000 - factorized_top_k/top_10_categorical_accuracy: 1.0000 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2.6279 - regularization_loss: 0.0000e+00 - total_loss: 2.6279\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 69ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1500 - factorized_top_k/top_5_categorical_accuracy: 0.7000 - factorized_top_k/top_10_categorical_accuracy: 1.0000 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2.5775 - regularization_loss: 0.0000e+00 - total_loss: 2.5775\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 69ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1500 - factorized_top_k/top_5_categorical_accuracy: 0.7000 - factorized_top_k/top_10_categorical_accuracy: 1.0000 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2.5548 - regularization_loss: 0.0000e+00 - total_loss: 2.5548\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 67ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1500 - factorized_top_k/top_5_categorical_accuracy: 0.7000 - factorized_top_k/top_10_categorical_accuracy: 1.0000 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2.5412 - regularization_loss: 0.0000e+00 - total_loss: 2.5412\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 65ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1500 - factorized_top_k/top_5_categorical_accuracy: 0.7000 - factorized_top_k/top_10_categorical_accuracy: 1.0000 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2.5319 - regularization_loss: 0.0000e+00 - total_loss: 2.5319\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 66ms/step - factorized_top_k/top_1_categorical_accuracy: 0.1500 - factorized_top_k/top_5_categorical_accuracy: 0.7000 - factorized_top_k/top_10_categorical_accuracy: 1.0000 - factorized_top_k/top_50_categorical_accuracy: 1.0000 - factorized_top_k/top_100_categorical_accuracy: 1.0000 - loss: 2.5253 - regularization_loss: 0.0000e+00 - total_loss: 2.5253\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2bcd59dc9d0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "# Predefined specializations\n",
    "specializations = [\n",
    "    \"Criminal Law\", \"Business Law\", \"Family Law\", \"Labor Law\", \n",
    "    \"Civil Rights Law\", \"Tax Law\", \"Real Estate Law\", \"Intellectual Property Law\", \n",
    "    \"Bankruptcy Law\", \"Personal Injury Law\", \"Environmental Law\", \"Estate Planning Law\", \n",
    "    \"Corporate Law\", \"Immigration Law\", \"Contract Law\"\n",
    "]\n",
    "\n",
    "# Generate synthetic data for lawyers\n",
    "def generate_lawyers(n=50):\n",
    "    return pd.DataFrame({\n",
    "        \"lawyer_id\": [f\"lawyer_{i+1}\" for i in range(n)],\n",
    "        \"specialization\": np.random.choice(specializations, n)\n",
    "    })\n",
    "\n",
    "# Generate synthetic data for clients\n",
    "def generate_clients(n=20):\n",
    "    return pd.DataFrame({\n",
    "        \"client_id\": [f\"client_{i+1}\" for i in range(n)],\n",
    "        \"preference\": np.random.choice(specializations, n)\n",
    "    })\n",
    "\n",
    "lawyers_df = generate_lawyers()\n",
    "clients_df = generate_clients()\n",
    "\n",
    "# Map specializations to integers\n",
    "specialization_to_id = {specialization: idx for idx, specialization in enumerate(specializations)}\n",
    "lawyers_df[\"specialization_id\"] = lawyers_df[\"specialization\"].map(specialization_to_id)\n",
    "clients_df[\"preference_id\"] = clients_df[\"preference\"].map(specialization_to_id)\n",
    "\n",
    "# Prepare the dataset for TensorFlow\n",
    "lawyers_dataset = tf.data.Dataset.from_tensor_slices(lawyers_df[\"specialization_id\"])\n",
    "clients_dataset = tf.data.Dataset.from_tensor_slices(clients_df[\"preference_id\"])\n",
    "\n",
    "embedding_dimension = 32\n",
    "\n",
    "# Lawyer and client models\n",
    "lawyer_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(len(specializations) + 1, embedding_dimension)\n",
    "])\n",
    "\n",
    "client_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(len(specializations) + 1, embedding_dimension)\n",
    "])\n",
    "\n",
    "# Define the TFRS model\n",
    "class LawyerRecommender(tfrs.Model):\n",
    "\n",
    "    def __init__(self, lawyer_model, client_model):\n",
    "        super().__init__()\n",
    "        self.lawyer_model = lawyer_model\n",
    "        self.client_model = client_model\n",
    "\n",
    "        # Retrieval task\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=lawyers_dataset.batch(128).map(self.lawyer_model)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        lawyer_embeddings = self.lawyer_model(features[\"specialization_id\"])\n",
    "        client_embeddings = self.client_model(features[\"preference_id\"])\n",
    "\n",
    "        return self.task(client_embeddings, lawyer_embeddings)\n",
    "\n",
    "# Create and compile the model\n",
    "model = LawyerRecommender(lawyer_model, client_model)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.5))\n",
    "\n",
    "# Interaction data (fake)\n",
    "interaction_data = tf.data.Dataset.from_tensor_slices({\n",
    "    \"specialization_id\": clients_df[\"preference_id\"],\n",
    "    \"preference_id\": clients_df[\"preference_id\"]\n",
    "}).batch(10)\n",
    "\n",
    "# Train the model\n",
    "model.fit(interaction_data, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
